"""
ì„ë² ë”© ìƒì„±ê¸°
LangChain Embeddingsë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
"""

from typing import List, Optional
import numpy as np
import hashlib
import re

from src.utils.logger import get_logger

logger = get_logger(__name__)


class EmbeddingGenerator:
    """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„° ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ìƒì„±ê¸°"""
    
    def __init__(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2", provider: str = "huggingface"):
        self.model_name = model_name
        self.provider = provider
        self.embedder = None
        self._initialize_embedder()
        logger.info(f"EmbeddingGenerator initialized: {provider}/{model_name}")
    
    def _initialize_embedder(self):
        """ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            if self.provider == "huggingface":
                try:
                    from langchain_community.embeddings import HuggingFaceEmbeddings
                except ImportError:
                    from langchain.embeddings import HuggingFaceEmbeddings
                self.embedder = HuggingFaceEmbeddings(
                    model_name=self.model_name,
                    model_kwargs={"device": "cpu"}
                )
            elif self.provider == "openai":
                try:
                    from langchain_openai import OpenAIEmbeddings
                except ImportError:
                    from langchain.embeddings import OpenAIEmbeddings
                import os
                self.embedder = OpenAIEmbeddings(
                    openai_api_key=os.getenv("OPENAI_API_KEY")
                )
            elif self.provider == "bedrock":
                try:
                    from langchain_community.embeddings import BedrockEmbeddings
                except ImportError:
                    from langchain.embeddings import BedrockEmbeddings
                import boto3
                bedrock_client = boto3.client("bedrock-runtime")
                self.embedder = BedrockEmbeddings(
                    client=bedrock_client,
                    model_id="amazon.titan-embed-text-v1"
                )
            else:
                raise ValueError(f"Unsupported provider: {self.provider}")

            logger.info(f"Embedder initialized successfully")

        except Exception as e:
            logger.warning(f"Embedder init failed ({e}), using Mock Embeddings")
            self.embedder = None
    
    # ------------------------------
    # ê³ ì • ì‹œë“œ ìƒì„±
    # ------------------------------
    def _stable_seed(self, text: str) -> int:
        """í…ìŠ¤íŠ¸ ê¸°ë°˜ ê³ ì • ì‹œë“œ ìƒì„± (Python hash ì‚¬ìš© ì ˆëŒ€ ê¸ˆì§€)"""
        md5 = hashlib.md5(text.encode()).hexdigest()
        return int(md5, 16) % (2**32)

    # ------------------------------
    # Mock ì„ë² ë”©
    # ------------------------------
    def _mock_embed(self, text: str, dimension: int = 384) -> List[float]:
        """Mock ì„ë² ë”©: ë‹¨ì–´ ê¸°ë°˜ + ê³ ì • í•´ì‹œ ê¸°ë°˜ ë²¡í„°"""

        # ê³µë°±/ê°œí–‰/ì§§ì€ í…ìŠ¤íŠ¸ë„ ê³ ìœ  ë²¡í„° ë¶€ì—¬
        if not text or len(text.strip()) < 2:
            np.random.seed(self._stable_seed(text))
            vec = np.random.normal(0, 0.1, dimension)
            vec = vec / np.linalg.norm(vec)
            return vec.tolist()

        # ì†Œë¬¸ì ë‹¨ì–´ ì¶”ì¶œ
        words = re.findall(r"\w+", text.lower())

        # ë‹¨ì–´ê°€ ì—†ëŠ” ê²½ìš°ë„ ê³ ì • ë²¡í„° ìƒì„±
        if not words:
            np.random.seed(self._stable_seed(text))
            vec = np.random.normal(0, 0.1, dimension)
            vec = vec / np.linalg.norm(vec)
            return vec.tolist()

        embedding = np.zeros(dimension)

        # ë‹¨ì–´ ê¸°ë°˜ ë¶„ì‚° ì„ë² ë”©
        for word in words:
            h = hashlib.md5(word.encode()).hexdigest()
            h_int = int(h, 16)

            # ê°ê°ì˜ ë‹¨ì–´ê°€ ì—¬ëŸ¬ ì°¨ì›ì— ë¶„ì‚° ê¸°ì—¬
            for i in range(20):
                idx = (h_int + i * 13) % dimension
                val = ((h_int >> (i * 5)) % 2000) / 1000.0 - 1.0
                embedding[idx] += val

        # ì •ê·œí™”
        norm = np.linalg.norm(embedding)

        # ğŸ”¥ Zero vector ë˜ëŠ” NaN/Inf ë°©ì§€
        if norm == 0 or np.isnan(norm) or np.isinf(norm):
            # ìƒˆë¡œìš´ ëœë¤ ë²¡í„° ìƒì„± (ì•ˆì „í•œ fallback)
            np.random.seed(self._stable_seed(text) + 999)
            embedding = np.random.normal(0, 0.5, dimension)
            embedding = embedding / np.linalg.norm(embedding)
        else:
            embedding = embedding / norm

        return embedding.tolist()


    # ------------------------------
    # Public API
    # ------------------------------
    def embed_text(self, text: str) -> List[float]:
        """ì§ˆë¬¸(ë‹¨ì¼ í…ìŠ¤íŠ¸) ì„ë² ë”©"""
        if self.embedder is None:
            return self._mock_embed(text)

        try:
            return self.embedder.embed_query(text)
        except Exception as e:
            logger.error(f"Embed query failed: {e}")
            return self._mock_embed(text)
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """ì—¬ëŸ¬ ë¬¸ì„œ ì„ë² ë”©"""
        if self.embedder is None:
            return [self._mock_embed(t) for t in texts]

        try:
            return self.embedder.embed_documents(texts)
        except Exception as e:
            logger.error(f"Embed documents failed: {e}")
            return [self._mock_embed(t) for t in texts]

    @property
    def dimension(self) -> int:
        """ì„ë² ë”© ì°¨ì›"""
        if self.embedder is None:
            return 384
        try:
            return len(self.embed_text("dimension_test"))
        except:
            return 384
