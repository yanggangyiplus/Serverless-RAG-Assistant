"""
RAG Pipeline (LangChain ìµœì‹  ë²„ì „ í˜¸í™˜ + ì™„ì „í•œ ë™ì‘ ë³´ì¥)
"""

from typing import Dict, Optional, List

import os
from src.utils.logger import get_logger
from .retriever import RAGRetriever

logger = get_logger(__name__)


class RAGPipeline:
    """
    RAG íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤
    LangChain ìµœì‹  ë²„ì „ì— ë§ì¶° ì „ì²´ êµ¬ì¡° ì¬êµ¬ì„±
    """
    
    def __init__(
        self,
        retriever: RAGRetriever,
        llm_provider: str = "mock",
        model_name: Optional[str] = None
    ):
        self.retriever = retriever
        self.llm_provider = llm_provider
        self.model_name = model_name or self._get_default_model(llm_provider)
        self.llm = self._initialize_llm()

        # ìµœì‹  RetrievalQA êµ¬ì„±
        self.chain = self._create_chain()

        logger.info(f"RAGPipeline initialized: provider={llm_provider}, model={self.model_name}")
    
    # ------------------------------------------------------------
    # ğŸ“Œ ê¸°ë³¸ ëª¨ë¸ ì´ë¦„
    # ------------------------------------------------------------
    def _get_default_model(self, provider: str) -> str:
        defaults = {
            "openai": "gpt-3.5-turbo",
            "bedrock": "amazon.titan-text-express-v1",
            "mock": "mock-model"
        }
        return defaults.get(provider, "mock-model")
    
    # ------------------------------------------------------------
    # ğŸ“Œ LLM ì´ˆê¸°í™” (OpenAI / Bedrock / Mock)
    # ------------------------------------------------------------
    def _initialize_llm(self):
        """LLM ë¡œë“œ. ì‹¤íŒ¨ ì‹œ mock LLM ì‚¬ìš©"""
        try:
            # ------------------------------
            # OpenAI
            # ------------------------------
            if self.llm_provider == "openai":
                try:
                    from langchain_openai import ChatOpenAI
                    api_key = os.getenv("OPENAI_API_KEY")
                    
                    if not api_key:
                        logger.warning("OPENAI_API_KEY ì—†ìŒ â†’ Mock LLM ì‚¬ìš©")
                        return self._create_mock_llm()
                    
                    return ChatOpenAI(
                        model_name=self.model_name,
                        temperature=0.0,
                        openai_api_key=api_key,
                    )
                except Exception as e:
                    logger.warning(f"OpenAI ì´ˆê¸°í™” ì‹¤íŒ¨: {e} â†’ Mock LLM ì‚¬ìš©")
                    return self._create_mock_llm()

            # ------------------------------
            # AWS Bedrock
            # ------------------------------
            elif self.llm_provider == "bedrock":
                try:
                    from langchain_community.llms import Bedrock
                    import boto3
                    client = boto3.client("bedrock-runtime")
                    return Bedrock(
                        client=client,
                        model_id=self.model_name,
                    )
                except Exception as e:
                    logger.warning(f"Bedrock ì´ˆê¸°í™” ì‹¤íŒ¨: {e} â†’ Mock LLM ì‚¬ìš©")
                    return self._create_mock_llm()

            # ------------------------------
            # Mock
            # ------------------------------
            else:
                logger.info("Mock LLM ì‚¬ìš©")
                return self._create_mock_llm()

        except Exception as e:
            logger.error(f"LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e} â†’ Mock LLM ì‚¬ìš©")
            return self._create_mock_llm()
    
    # ------------------------------------------------------------
    # ğŸ“Œ Mock LLM (LangChain ìµœì‹  ë²„ì „ í˜¸í™˜)
    # ------------------------------------------------------------
    def _create_mock_llm(self):
        """
        LangChain 0.1~0.2 êµ¬ì¡°ì— ì™„ë²½ ëŒ€ì‘í•˜ëŠ” Mock LLM
        """
        from typing import Any, List, Optional

        # ìµœì‹  LLM Base
        try:
            from langchain_core.language_models.llms import BaseLLM
            from langchain_core.outputs import LLMResult
            from langchain_core.callbacks.manager import CallbackManagerForLLMRun
        except ImportError:
            # ì•„ì£¼ êµ¬ë²„ì „ ëŒ€ì‘
            class SimpleLLM:
                def __call__(self, prompt):
                    return f"[Mock] {prompt[:150]}..."
            return SimpleLLM()

        class MockLLM(BaseLLM):

            @property
            def _llm_type(self) -> str:
                return "mock-llm"
            
            def _call(self, prompt: str, stop=None) -> str:
                return f"[Mock Response] {prompt[:200]}..."

            def _generate(
                self,
                prompts: List[str],
                stop: Optional[List[str]] = None,
                run_manager: Optional[CallbackManagerForLLMRun] = None,
                **kwargs: Any,
            ) -> LLMResult:
                generations = [[{"text": self._call(prompt)}] for prompt in prompts]
                return LLMResult(generations=generations)

        return MockLLM()
    
    # ------------------------------------------------------------
    # ğŸ“Œ RetrievalQA Chain ìƒì„±
    # ------------------------------------------------------------
    def _create_chain(self):
        """
        RetrievalQA ì²´ì¸ ìƒì„±
        LangChain ìµœì‹ ë²„ì „ì—ì„œ ì™„ì „íˆ í˜¸í™˜ë˜ë„ë¡ êµ¬ì„±
        """
        try:
            from langchain.chains.retrieval_qa.base import RetrievalQA
            from langchain_core.prompts import PromptTemplate
        except Exception as e:
            logger.warning(f"RetrievalQA import ì‹¤íŒ¨: {e} â†’ Fallback mode")
            return None

        template = """ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”.
ë¬¸ì„œì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ "ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë§í•˜ì„¸ìš”.

ë¬¸ì„œ:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""

        PROMPT = PromptTemplate(
            template=template,
            input_variables=["context", "question"]
        )

        try:
            chain = RetrievalQA.from_chain_type(
                llm=self.llm,
                chain_type="stuff",
                retriever=self.retriever,
                return_source_documents=True,
                chain_type_kwargs={"prompt": PROMPT}
            )
            return chain

        except Exception as e:
            logger.error(f"RetrievalQA ì²´ì¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    # ------------------------------------------------------------
    # ğŸ“Œ QUERY ìˆ˜í–‰ (í•µì‹¬)
    # ------------------------------------------------------------
    def query(self, question: str) -> Dict:
        """
        ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±
        ìµœì‹  LangChain êµ¬ì¡°ì— ì™„ì „íˆ ë§ì¶° ìˆ˜í–‰ë˜ë„ë¡ ìˆ˜ì •ë¨.
        """
        try:
            # ì²´ì¸ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš° fallback
            if self.chain is None:
                logger.warning("RetrievalQA chain ì—†ìŒ â†’ fallback ìˆ˜í–‰")
                docs = self.retriever.get_relevant_documents(question)

                if docs:
                    context = "\n".join([d.page_content for d in docs[:3]])
                    answer = f"[Mock Fallback] {context[:300]}"
                else:
                    answer = "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

                return {
                    "answer": answer,
                    "source_documents": [
                        {"content": d.page_content, "metadata": d.metadata}
                        for d in docs
                    ],
                }

            # ------------------------------
            # ğŸ”¥ ìµœì‹  RetrievalQA ì…ë ¥ í‚¤ëŠ” "question"
            # ------------------------------
            result = self.chain({"question": question})

            # LangChain êµ¬ì¡°
            answer = result.get("result", "")
            source_docs = result.get("source_documents", [])

            converted = [
                {
                    "content": d.page_content,
                    "metadata": d.metadata,
                }
                for d in source_docs
            ]

            return {
                "answer": answer,
                "source_documents": converted,
            }

        except Exception as e:
            logger.error(f"RAGPipeline ì˜¤ë¥˜: {e}", exc_info=True)
            return {
                "answer": f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                "source_documents": []
            }
